# 확률론 맛보기

## 목표

- [ ] 확률변수, 조건부확률, 기댓값 등은 확률론의 매우 기초적인 내용으로 정확히 이해하기
- [ ] 확률본포를 모를 때 => 몬테카를로 방법을 통해 기댓값 계산
- [ ] 몬테카를로 방법을 활용하여 원주율에 대한 근삿값 구하는 방법?
- [ ] 용어를 자주 접해보자



### 확률론이 왜 필요할까?

- 딥러닝은 확률론 기반의 기계학습 이론에 바탕을 둔다
- 기계학습에서 사용되는 loss function들의 작동 원리는 데이터 공간을 통계적으로 해석해서 유도하게 된다
- 회귀 분석에서 L2-노름은 **예측오차의 분산을 가장 최소화하는 방향**으로 학습하도록 유도
- 분류 문제에서 교차엔트로피(cross-entropy)는 **모델 예측의 불확실성을 최소화하는 방향**으로 학습하도록 유도
- 분산 및 불확실성을 **최소화하기 위해서는 측정하는 방법**을 알아야 한다



###  확률분포

- 데이터의 초상화
- 데이터공간을 X * Y 라 표기하고 D는 데이터공간에서 데이터를 추출하는 분포

<img src="C:\Users\98dls\AppData\Roaming\Typora\typora-user-images\image-20210806030103514.png" alt="image-20210806030103514" style="zoom:150%;" />

- 데이터는 확률변수로 (x, y) ~ D라 표기
- 결합분포 P(x, y)는 D를 모델링한다
- P(x) 는 입력 x 에 대한 주변확률분포로 y에 대한 정보를 주지 않는다

![image-20210806030200386](C:\Users\98dls\AppData\Roaming\Typora\typora-user-images\image-20210806030200386.png)

- 조건부 확률분포 P(x|y)는 데이터 공간에서 입력x 와 출력 y사이의 관게를 모델링



### 이산확률변수 vs 연속확률변수

- 데이터공간이 아닌 확률변수의 확률분포 D에 따라 결정된다. (실수형이라고 연속형이고 정수형이라 이산형은 아님)

- 이산형 확률변수는 확률변수가 가질 수 있는 경우의 수를 모두 고려하여 확률을 더해서 모델링

- P(X = x) => 확률변수가 x값을 가질 확률 => 확률 질량 함수라고 불리기도 한다

  ![image-20210806025503846](C:\Users\98dls\AppData\Roaming\Typora\typora-user-images\image-20210806025503846.png)

- 연속형 확률 변수는 데이터 공간에 정의된 확률변수의 밀도(density) 위에서 적분을 통해 모델링

![image-20210806025718779](C:\Users\98dls\AppData\Roaming\Typora\typora-user-images\image-20210806025718779.png)

<img src="C:\Users\98dls\AppData\Roaming\Typora\typora-user-images\image-20210806025746309.png" alt="image-20210806025746309" style="zoom:150%;" />

- 모든 확률변수가 이산형 또는 연속형으로 구별하는 것이 아니니 주의



### 조건부확률과 기계학습

- 조건부확률 P(y|x) 는 입력변수 x에 대해 정답이 y일 확률
- 로지스틱 회귀에서 사용했던 선형모델과 소프트맥스 함수의 결합은 데이터에서 추출된 패턴을 기반으로 확률을 해석



### 몬테카를로

- 확률분포를 모를 때 샘플링을 통해 기대값을 구하는 방법 (이산형, 연속형 관계없이 사용 가능)
